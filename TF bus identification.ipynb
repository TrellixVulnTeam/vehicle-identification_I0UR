{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQCnYPVDrsgx"
   },
   "source": [
    "Based on:\n",
    "\n",
    "https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/\n",
    "\n",
    "https://towardsdatascience.com/deeppicar-part-6-963334b2abe0\n",
    "\n",
    "\n",
    "Pretrained Model from (Tensorflow detection model zoo):\n",
    "\n",
    "https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOKeb3tRElm7"
   },
   "source": [
    "# Section 0: Actions done before running this notebook\n",
    "1- Took around 300 photos of cars with lables\n",
    "\n",
    "2- Run script to scale images dimentions to 800x600 and Divide images to 80% train, 10% test, 10% validation\n",
    "\n",
    "3- Use [labelImg](https://github.com/tzutalin/labelImg) for annotating \"plate no\" and \"bus run\" objects on each of train & test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7404,
     "status": "ok",
     "timestamp": 1588204304662,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "MVrVzjhp6Tnz",
    "outputId": "7bfb61e2-179a-4ccc-e2bc-55b698259bae"
   },
   "outputs": [],
   "source": [
    "# These versions required to work with the scripts available in https://github.com/tensorflow/models/tree/master/research/object_detection\n",
    "# for transfer learning\n",
    "!pip install numpy==1.17.4\n",
    "%tensorflow_version 1.x\n",
    "!pip install --user gast==0.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etOThFKokSmU"
   },
   "source": [
    "# Section 1: Mount Google drive\n",
    "Mount my Google Drive to save modeling output files there, so that it won't be wiped out when colab Virtual Machine restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41051,
     "status": "ok",
     "timestamp": 1588204349807,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "eCjkgwJA-Zyx",
    "outputId": "c25a33b3-77d5-4bfe-cca9-d46c77df5a22"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "model_dir = '/content/gdrive/My Drive/UniMelb/Semester1_2020/Internship/SilverPond/FinalSource'\n",
    "#!rm -rf '{model_dir}'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "!ls -ltra '{model_dir}'/.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yhzxsJb3dpWq"
   },
   "source": [
    "# Section 2: Configs and Hyperparameters\n",
    "\n",
    "Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLtZc41d-rrI"
   },
   "outputs": [],
   "source": [
    "# Number of training steps\n",
    "num_steps = 5000  # 200000\n",
    "\n",
    "# Number of evaluation steps\n",
    "num_eval_steps = 50\n",
    "\n",
    "\n",
    "# model name and configs are from Model Zoo github: \n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
    "# https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs\n",
    "MODELS_CONFIG = {\n",
    "    'ssdlite_mobilenet_v2': {\n",
    "        'model_name': 'ssdlite_mobilenet_v2_coco_2018_05_09',\n",
    "        'pipeline_file': 'ssdlite_mobilenet_v2_coco.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    'ssd_mobilenet_v2_quantized': {\n",
    "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
    "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz\n",
    "    'ssd_mobilenet_v3_small_coco': {\n",
    "        'model_name': 'ssd_mobilenet_v3_small_coco_2020_01_14',\n",
    "        'pipeline_file': 'ssdlite_mobilenet_v3_small_320x320_coco.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    #https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssdlite_mobilenet_v3_large_320x320_coco.config\n",
    "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz\n",
    "    'ssd_mobilenet_v3_large_coco': {\n",
    "        'model_name': 'ssd_mobilenet_v3_large_coco_2020_01_14',\n",
    "        'pipeline_file': 'ssdlite_mobilenet_v3_large_320x320_coco.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    #https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/ssdlite_mobilenet_edgetpu_coco_quant.tar.gz\n",
    "    'ssdlite_mobilenet_edgetpu_coco_quant': {\n",
    "        'model_name': 'ssdlite_mobilenet_edgetpu_coco_quant',\n",
    "        'pipeline_file': 'ssdlite_mobilenet_edgetpu_320x320_coco_quant.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    'faster_rcnn_inception_v2': {\n",
    "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
    "        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n",
    "        'batch_size': 12\n",
    "    },\n",
    "    'rfcn_resnet101': {\n",
    "        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n",
    "        'pipeline_file': 'rfcn_resnet101_pets.config',\n",
    "        'batch_size': 12\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select a model in MODELS_CONFIG\n",
    "# Note: Must be a quantized model, which reduces the model size significantly for mobile/edge use\n",
    "selected_model = 'ssd_mobilenet_v2_quantized'\n",
    "#selected_model = 'ssd_mobilenet_v3_large_coco'  # used 15K training steps to get better results\n",
    "#selected_model = 'ssd_mobilenet_v3_small_coco'  # didn't give good results\n",
    "#selected_model = 'ssdlite_mobilenet_v2'  # the False Positives for plate numbers were almost in every images\n",
    "\n",
    "# Name of the object detection model to use.\n",
    "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
    "\n",
    "# Name of the pipline file in tensorflow object detection API.\n",
    "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
    "\n",
    "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
    "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rw-YqZHUKv-Y"
   },
   "source": [
    "# Section 3: Set up Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1350,
     "status": "ok",
     "timestamp": 1588206281625,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "2Bh7-Xo6JUFt",
    "outputId": "7b6a55a7-74cf-4046-b582-0d1808094e0c"
   },
   "outputs": [],
   "source": [
    "# Where the new trained model will be saved\n",
    "from datetime import datetime\n",
    "curdate = datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "trainedmodel_dir = os.path.join(model_dir, 'trainedmodel', selected_model, curdate)\n",
    "os.makedirs(trainedmodel_dir, exist_ok = True)\n",
    "\n",
    "print(trainedmodel_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bI8__uNS8-ns"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15141,
     "status": "ok",
     "timestamp": 1588168002566,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "sMyfqlDB_TA2",
    "outputId": "f6da4b3b-5496-4313-e0fe-60e6417b8aa7"
   },
   "outputs": [],
   "source": [
    "%cd '{model_dir}'\n",
    "#!git clone --quiet https://github.com/tensorflow/models.git\n",
    "print('installing protobuf')\n",
    "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
    "print('installing Cython')\n",
    "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
    "print('installing pycocotools')\n",
    "!pip install -q pycocotools\n",
    "print('run protoc')\n",
    "%cd '{model_dir}/models/research'\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "print('setting environment variable')\n",
    "import os\n",
    "os.environ['PYTHONPATH'] += ':/content/gdrive/My Drive/UniMelb/Semester1_2020/Internship/SilverPond/FinalSource/models/research/:/content/gdrive/My Drive/UniMelb/Semester1_2020/Internship/SilverPond/FinalSource/models/research/slim/'\n",
    "\n",
    "print('run model_builder_test')\n",
    "# To verify all dependencies are successfull installed\n",
    "#!python object_detection/builders/model_builder_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u-k7uGThXlny"
   },
   "source": [
    "## Prepare `tfrecord` files\n",
    "\n",
    "Use the following scripts to generate the `tfrecord` files which is used for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 289428,
     "status": "ok",
     "timestamp": 1588168289599,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "ezGDABRXXhPP",
    "outputId": "ec559f52-46e2-498a-fa64-320bdcdb8714"
   },
   "outputs": [],
   "source": [
    "%cd {model_dir}\n",
    "print(\"train_cars xml_to_csv\")\n",
    "# Convert train folder annotation xml files to a single csv file,\n",
    "# generate the `label_map.pbtxt` file to `data/` directory as well.\n",
    "!python code/xml_to_csv.py -i Data/train_cars -o Data/annotations/train_labels.csv -l Data/annotations\n",
    "print(\"test_cars xml_to_csv\")\n",
    "# Convert test folder annotation xml files to a single csv.\n",
    "!python code/xml_to_csv.py -i Data/test_cars -o Data/annotations/test_labels.csv\n",
    "\n",
    "print(\"train_labels generate_tfrecord\")\n",
    "# Generate `train.record`\n",
    "!python code/generate_tfrecord.py --csv_input=Data/annotations/train_labels.csv --output_path=Data/annotations/train.record --img_path=Data/train_cars --label_map Data/annotations/label_map.pbtxt\n",
    "print(\"test_labels generate_tfrecord\")\n",
    "# Generate `test.record`\n",
    "!python code/generate_tfrecord.py --csv_input=Data/annotations/test_labels.csv --output_path=Data/annotations/test.record --img_path=Data/test_cars --label_map Data/annotations/label_map.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgd-fzAIkZlV"
   },
   "outputs": [],
   "source": [
    "test_record_fname = model_dir + '/Data/annotations/test.record'\n",
    "train_record_fname = model_dir + '/Data/annotations/train.record'\n",
    "label_map_pbtxt_fname = model_dir + '/Data/annotations/label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 239716,
     "status": "ok",
     "timestamp": 1588168293301,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "rZ8otnfyCYdG",
    "outputId": "b63a0b60-3eeb-449a-d53b-a45bfe2b33d6"
   },
   "outputs": [],
   "source": [
    "!cat Data/annotations/test_labels.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCNYAaC7w6N8"
   },
   "source": [
    "## Download base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1588174796098,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "fbmzgLsOjhMs",
    "outputId": "73e0a19f-9a3f-4367-d3b5-02c2506c806f"
   },
   "outputs": [],
   "source": [
    "%cd '{model_dir}/models/research'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3239,
     "status": "ok",
     "timestamp": 1588174800483,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "orDCj6ihgUMR",
    "outputId": "3fd000ba-3617-4e17-94d6-b86ecb54cff1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import urllib.request\n",
    "import tarfile\n",
    "MODEL_FILE = MODEL + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "DEST_DIR = model_dir+'/models/research/pretrained_model'\n",
    "\n",
    "# commented if already done\n",
    "#'''\n",
    "print('MODEL_FILE=', MODEL_FILE)\n",
    "if not (os.path.exists(MODEL_FILE)):\n",
    "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "\n",
    "print('extracting model file')\n",
    "tar = tarfile.open(MODEL_FILE)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "os.remove(MODEL_FILE)\n",
    "if (os.path.exists(DEST_DIR)):\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "\n",
    "os.rename(MODEL, DEST_DIR)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4702,
     "status": "ok",
     "timestamp": 1588168327910,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "DWby7DIT1AUh",
    "outputId": "a04a496a-785a-4250-8fa7-4b34eeec06ab"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7787,
     "status": "ok",
     "timestamp": 1588174810109,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "pGhvAObeiIix",
    "outputId": "6dd1d32a-994f-4744-ba6d-d46ae00ed4c7"
   },
   "outputs": [],
   "source": [
    "!echo '{DEST_DIR}'\n",
    "!ls -alh '{DEST_DIR}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4882,
     "status": "ok",
     "timestamp": 1588174810110,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "UHnxlfRznPP3",
    "outputId": "0a7b06c3-4f6e-43f7-d494-aece29a4cf66"
   },
   "outputs": [],
   "source": [
    "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
    "fine_tune_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aYW8J5JoLP4I"
   },
   "source": [
    "# Section 4: Transfer Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvwtHlLOeRJD"
   },
   "source": [
    "## Configuring a Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22901,
     "status": "ok",
     "timestamp": 1588186309824,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "Fa58Txyp6Qut",
    "outputId": "bd05f1c9-58d3-45c6-eadf-a4a5582fcb31"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_fname = os.path.join(model_dir+'/models/research/object_detection/samples/configs/', pipeline_file)\n",
    "print(pipeline_fname)\n",
    "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fG1nCNpUXcRU"
   },
   "outputs": [],
   "source": [
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqbFyhwH6Zzl"
   },
   "outputs": [],
   "source": [
    "# model's .config file is protocol buffer file so can be edited with via google.protobuf\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "\n",
    "# read pipeline config file\n",
    "with tf.gfile.GFile(pipeline_fname, \"r\") as f:\n",
    "  proto_str = f.read()\n",
    "  text_format.Merge(proto_str, pipeline_config)\n",
    "\n",
    "# pipeline_config will have all config parameters that can be overwritten\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[0] = train_record_fname\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0] = test_record_fname\n",
    "\n",
    "pipeline_config.train_input_reader.label_map_path = label_map_pbtxt_fname\n",
    "pipeline_config.eval_input_reader[0].label_map_path = label_map_pbtxt_fname\n",
    "\n",
    "if pipeline_config.train_config.fine_tune_checkpoint:\n",
    "  pipeline_config.train_config.fine_tune_checkpoint = fine_tune_checkpoint\n",
    "pipeline_config.train_config.batch_size = batch_size\n",
    "pipeline_config.train_config.num_steps = num_steps\n",
    "\n",
    "# Depending on the base model type, currently there is either ssd or faster_rcnn\n",
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
    "if pipeline_config.model.ssd:\n",
    "  pipeline_config.model.ssd.num_classes = num_classes\n",
    "elif pipeline_config.model.faster_rcnn:\n",
    "  pipeline_config.model.faster_rcnn.num_classes = num_classes\n",
    "\n",
    "# Save updated config\n",
    "config_text = text_format.MessageToString(pipeline_config)\n",
    "with tf.gfile.Open(pipeline_fname, \"wb\") as f:\n",
    "    f.write(config_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6121,
     "status": "ok",
     "timestamp": 1588174830522,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "0IH96bbydOWn",
    "outputId": "11055096-9b48-4942-833e-f406e3463df9"
   },
   "outputs": [],
   "source": [
    "!cat '{label_map_pbtxt_fname}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7906,
     "status": "ok",
     "timestamp": 1588174835023,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "0F3yKiGVkRLs",
    "outputId": "d414894a-526c-4909-f1bf-2369201325cd"
   },
   "outputs": [],
   "source": [
    "# look for num_classes: 6, since we have 5 different road signs and 1 person type (total of 6 types) \n",
    "!cat '{pipeline_fname}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23TECXvNezIF"
   },
   "source": [
    "## Run Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8971,
     "status": "ok",
     "timestamp": 1588174846907,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "S7ZACJXsBtSc",
    "outputId": "68bd9b3e-3083-4202-b3ca-7f36b480b274"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10665,
     "status": "ok",
     "timestamp": 1588168476398,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "JOy7E3Hm7Qbf",
    "outputId": "6999b369-491f-4e17-8325-dbc409d2f145"
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G8o6r1o5SC5M"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = model_dir#+'Rerun'\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3196,
     "status": "ok",
     "timestamp": 1588168476399,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "bxxtJ56mwsPG",
    "outputId": "f79eb3fd-5e49-4d91-f90e-2e4767b2f84d"
   },
   "outputs": [],
   "source": [
    "LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ge1OX7gcSC7S"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 6006 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m5GSGxZNh8rp"
   },
   "source": [
    "### Get Tensorboard link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6454,
     "status": "ok",
     "timestamp": 1588174849657,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "rzxm3ljMb85p",
    "outputId": "f306aef8-ef94-40eb-9252-788f703b1b9f"
   },
   "outputs": [],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDddx2rPfex9"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZc0RFcUjTa-"
   },
   "source": [
    "Now all inputs are set up, just train the model.   This process may take a few hours.   Since we are saving the model training results (model.ckpt-* files) in our google drive (a persistent storage that will survice the restart of our colab VM instance), we can safely leave and return a few hours later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QvUx8uoxcR_W"
   },
   "outputs": [],
   "source": [
    "#################### SEND ALERT EMAIL AT FINISH WITH GMAIL OPTIONAL #####################\n",
    "# To send email from Python from your google account, MUST \n",
    "# 1) Enable less secure app\n",
    "# https://myaccount.google.com/lesssecureapps\n",
    "# 2) Disable Unlock Capcha\n",
    "# https://accounts.google.com/b/0/DisplayUnlockCaptcha\n",
    "\n",
    "import smtplib\n",
    "\n",
    "def SendEmail(msg):\n",
    "    with open('/content/gdrive/My Drive/Colab Notebooks/pw.txt') as file:\n",
    "        data = file.readlines()\n",
    "        \n",
    "    gmail_user = 'gehmaid@student.unimelb.edu.au'  \n",
    "    gmail_password = data[0]\n",
    "\n",
    "\n",
    "    sent_from = gmail_user  \n",
    "    to = ['gehmaid@student.unimelb.edu.au']  \n",
    "    subject = msg  \n",
    "    body = '%s\\n\\n- Ghawady' % msg\n",
    "\n",
    "    email_text = \\\n",
    "\"\"\"From: %s\n",
    "To: %s\n",
    "Subject: %s\n",
    "\n",
    "%s\n",
    "\"\"\" % (sent_from, \", \".join(to), subject, body)\n",
    "\n",
    "    server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    server.login(gmail_user, gmail_password)\n",
    "    server.sendmail(sent_from, to, email_text)\n",
    "    server.quit()\n",
    "\n",
    "    print(f'Email: \\n{email_text}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1588186309825,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "hlOFty-1TYOw",
    "outputId": "7ce49eea-126c-4835-bc97-2e8998086e51"
   },
   "outputs": [],
   "source": [
    "num_steps = 5000\n",
    "SendEmail(\"Colab train started\")\n",
    "!python '{model_dir}'/models/research/object_detection/model_main.py \\\n",
    "    --pipeline_config_path='{pipeline_fname}' \\\n",
    "    --model_dir='{trainedmodel_dir}' \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps='{num_steps}' \\\n",
    "    --num_eval_steps='{num_eval_steps}'\n",
    "SendEmail(\"Colab train finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 909401,
     "status": "ok",
     "timestamp": 1588169426877,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "Gp1KqA2JcZN7",
    "outputId": "65456bcd-96b8-4960-fa8b-383695fb0699"
   },
   "outputs": [],
   "source": [
    "!ls -ltra '{trainedmodel_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4Kzh3_JLVW-"
   },
   "source": [
    "# Section 5: Save and Convert Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmSESMetj1sa"
   },
   "source": [
    "## Exporting a Trained Inference Graph\n",
    "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r5eIS-RNVIRx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "output_directory = trainedmodel_dir + '/fine_tuned_model'\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "!ls '{output_directory}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1389,
     "status": "ok",
     "timestamp": 1588186309826,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "6R0Yz7tIVO9C",
    "outputId": "205562e0-d25e-407e-ad3d-b332f3c21610"
   },
   "outputs": [],
   "source": [
    "lst = os.listdir(trainedmodel_dir)\n",
    "# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
    "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
    "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
    "last_model = lst[steps.argmax()].replace('.meta', '')\n",
    "\n",
    "last_model_path = os.path.join(trainedmodel_dir, last_model)\n",
    "print(last_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1328,
     "status": "ok",
     "timestamp": 1588186309827,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "90CjhkueVRpi",
    "outputId": "4a0d61ed-6afa-4af7-c6ed-383a2c5ac66f"
   },
   "outputs": [],
   "source": [
    "!echo creates the frozen inference graph in fine_tune_model\n",
    "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
    "\n",
    "!python '{model_dir}'/models/research/object_detection/export_inference_graph.py \\\n",
    "    --input_type=image_tensor \\\n",
    "    --pipeline_config_path='{pipeline_fname}' \\\n",
    "    --output_directory='{output_directory}' \\\n",
    "    --trained_checkpoint_prefix='{last_model_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1277,
     "status": "ok",
     "timestamp": 1588186309827,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "SHcirc8qVa04",
    "outputId": "82183cc5-6e11-4c2b-ae15-4f77da8c3cc4"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n",
    "# create the tensorflow lite graph\n",
    "!python '{model_dir}'/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
    "    --pipeline_config_path='{pipeline_fname}' \\\n",
    "    --trained_checkpoint_prefix='{last_model_path}' \\\n",
    "    --output_directory='{output_directory}' \\\n",
    "    --add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1209,
     "status": "ok",
     "timestamp": 1588186309827,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "cko618o2VeAA",
    "outputId": "236e9583-1541-4b75-dda7-b8c7c5146107"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n",
    "# create the tensorflow lite graph\n",
    "!python '{model_dir}'/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
    "    --pipeline_config_path='{pipeline_fname}' \\\n",
    "    --trained_checkpoint_prefix='{last_model_path}' \\\n",
    "    --output_directory='{output_directory}' \\\n",
    "    --add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1148,
     "status": "ok",
     "timestamp": 1588186309828,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "BAkuZNHtVimx",
    "outputId": "f415bca1-fd53-4e81-fe06-30fd0c5a0d04"
   },
   "outputs": [],
   "source": [
    "!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n",
    "!tflite_convert \\\n",
    "  --output_file='{output_directory}/detect.tflite' \\\n",
    "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
    "  --inference_type=QUANTIZED_UINT8 \\\n",
    "  --input_arrays='normalized_input_image_tensor' \\\n",
    "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
    "  --mean_values=128 \\\n",
    "  --std_dev_values=128 \\\n",
    "  --input_shapes=1,300,300,3 \\\n",
    "  --change_concat_input_ranges=false \\\n",
    "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
    "  --allow_custom_ops\n",
    "  #--output_arrays represent four arrays: detection_boxes, detection_classes, detection_scores, and num_detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1588186309828,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "uTikaJKpK6No",
    "outputId": "b4d2ab89-f514-4f55-d943-dd4a3aa3aac6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n",
    "!tflite_convert \\\n",
    "  --output_file='{output_directory}/bus_labels_quantized.tflite' \\\n",
    "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
    "  --inference_type=QUANTIZED_UINT8 \\\n",
    "  --input_arrays='normalized_input_image_tensor' \\\n",
    "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
    "  --mean_values=128 \\\n",
    "  --std_dev_values=128 \\\n",
    "  --input_shapes=1,300,300,3 \\\n",
    "  --change_concat_input_ranges=false \\\n",
    "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
    "  --allow_custom_ops\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1588186309829,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "i_gFUwgtVwzL",
    "outputId": "faacb1cb-eb8a-47c2-8816-a8fd949f04f7"
   },
   "outputs": [],
   "source": [
    "print(output_directory)\n",
    "!ls -ltra '{output_directory}'\n",
    "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")  # this is tflite graph\n",
    "!cp '{label_map_pbtxt_fname}' '{output_directory}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mz1gX19GlVW7"
   },
   "source": [
    "# Section 6: Run inference test\n",
    "Test with images in repository `object_detection/data/images/test` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvusjJTyt7Ol"
   },
   "source": [
    "### Reinitialize required variables incase this notebook is not run from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1588186309829,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "7GFqouhIU69u",
    "outputId": "0f051431-e474-4aa5-8bf1-18750ce12414"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%tensorflow_version 1.x\n",
    "!pip install numpy==1.17.4\n",
    "!pip install --user gast==0.2.2\n",
    "#!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1588186309829,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "2lQhynkoVNwE",
    "outputId": "85ed2a6f-9f29-4a20-8053-c1b0d97ee885"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "model_dir = '/content/gdrive/My Drive/UniMelb/Semester1_2020/Internship/SilverPond/FinalSource'\n",
    "\n",
    "test_record_fname = model_dir + '/Data/annotations/test.record'\n",
    "train_record_fname = model_dir + '/Data/annotations/train.record'\n",
    "label_map_pbtxt_fname = model_dir + '/Data/annotations/label_map.pbtxt'\n",
    "num_classes = 2\n",
    "\n",
    "selected_model = 'ssd_mobilenet_v3_small_coco' #'ssd_mobilenet_v2_quantized'\n",
    "curdate = '2020_04_29'\n",
    "trainedmodel_dir = os.path.join(model_dir, 'trainedmodel', selected_model, curdate)\n",
    "os.makedirs(trainedmodel_dir, exist_ok = True)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwreaQQHtR8Q"
   },
   "source": [
    "### Load model & test images for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1588186309830,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "Rpbo5HC0VXuU",
    "outputId": "f419a006-32e0-49e8-91e1-cbdd1129750b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "output_directory = trainedmodel_dir+'/fine_tuned_model'\n",
    "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")  # this is tflite graph\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = pb_fname\n",
    "print(PATH_TO_CKPT)\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = label_map_pbtxt_fname\n",
    "\n",
    "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
    "PATH_TO_TEST_IMAGES_DIR =  os.path.join(model_dir, \"Data/val_cars\")\n",
    "print(PATH_TO_TEST_IMAGES_DIR)\n",
    "\n",
    "assert os.path.isfile(pb_fname)\n",
    "assert os.path.isfile(PATH_TO_LABELS)\n",
    "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))\n",
    "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
    "print(TEST_IMAGE_PATHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXskuNA6vVFw"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 878,
     "status": "ok",
     "timestamp": 1588186309832,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "NR86icuNVb_-",
    "outputId": "73b56da2-1aff-4596-9fa8-f60748b719f9"
   },
   "outputs": [],
   "source": [
    "%cd '{model_dir}'/models/research/object_detection\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {\n",
    "                output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                        tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(\n",
    "                    tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(\n",
    "                    tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(\n",
    "                    tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
    "                                           real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
    "                                           real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "            #print('output_dict',output_dict)\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(\n",
    "                output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "            \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wH28wm2D9NjD"
   },
   "source": [
    "#### Draw bounding boxes on all test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1oiv6JUOovDYbEJGtnyOlURWYKZ_-wywQ"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1588186309832,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "oOQLvdrJVfwW",
    "outputId": "6b1bf7d1-e1d8-4725-94ac-7ccebbd09bf7"
   },
   "outputs": [],
   "source": [
    "# running inferences.  This should show images with bounding boxes\n",
    "%matplotlib inline\n",
    "\n",
    "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    #print('Returned output_dict',output_dict)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks'),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=2)\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DvXEYmuvjcZ"
   },
   "source": [
    "### Bounding box image crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmxqsYEIC1IN"
   },
   "outputs": [],
   "source": [
    "def get_bounding_box_coordinates(image,\n",
    "                               ymin,\n",
    "                               xmin,\n",
    "                               ymax,\n",
    "                               xmax,\n",
    "                               use_normalized_coordinates=True):\n",
    "  \n",
    "  im_width, im_height = image.size\n",
    "  if use_normalized_coordinates:\n",
    "    (left, right, top, bottom) = (int(xmin * im_width), int(xmax * im_width),\n",
    "                                  int(ymin * im_height), int(ymax * im_height))\n",
    "  else:\n",
    "    (left, right, top, bottom) = (xmin, xmax, ymin, ymax)\n",
    "\n",
    "  return (left, top, right, bottom)  # tuple order matters for cropping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l8DyMT06xqMl"
   },
   "source": [
    "### Read Text from cropped image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeWqdmfxxCjf"
   },
   "source": [
    "#### Use pytesseract for Text Read (Bad)\n",
    "\n",
    "config reference:\n",
    "\n",
    "https://www.pyimagesearch.com/2018/09/17/opencv-ocr-and-text-recognition-with-tesseract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 585,
     "status": "ok",
     "timestamp": 1588186309835,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "U6o_cn9UHrnz",
    "outputId": "49ea82be-50ff-429c-cfd5-18bcb368e242"
   },
   "outputs": [],
   "source": [
    "!apt install tesseract-ocr\n",
    "!apt install libtesseract-dev\n",
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfLWKdndp1Dj"
   },
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import cv2\n",
    "\n",
    "import re\n",
    "\n",
    "def read_image_text(img):\n",
    "  print('image', type(img))\n",
    "  img_np = load_image_into_numpy_array(img)\n",
    "  crop = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "  \n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(crop)\n",
    "  crop = cv2.GaussianBlur(crop, (5, 5), 0)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(crop)\n",
    "  #_, crop = cv2.threshold(crop, 100, 150, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "  #plt.figure(figsize=IMAGE_SIZE)\n",
    "  #plt.imshow(crop)\n",
    "\n",
    "  print('1=',pytesseract.image_to_string(crop, config='-l eng --oem 1 --psm 7'))\n",
    "  print('2=',pytesseract.image_to_string(crop, config='--oem 3 --psm 3'))\n",
    "  print('3=',pytesseract.image_to_string(crop))\n",
    "  print('org 3=',pytesseract.image_to_data(crop, output_type=Output.DICT))\n",
    "  print('org 1=',pytesseract.image_to_string(image_np, config='-l eng --oem 1 --psm 7'))\n",
    "  print('org 2=',pytesseract.image_to_string(image_np, config='--oem 3 --psm 3'))\n",
    "  print('org 3=',pytesseract.image_to_string(image_np))\n",
    "  print('org 3=',pytesseract.image_to_data(image_np, output_type=Output.DICT))\n",
    "\n",
    "  custom_config = r'-l eng --oem 1 --psm 7' #r'--oem 3 --psm 3'\n",
    "  text = pytesseract.image_to_string(img, config=custom_config)\n",
    "  print('text from image', text)\n",
    "  return text\n",
    "\n",
    "\n",
    "def read_image_text(img):\n",
    "  (left, top, right, bottom) = (0,25,img.size[0],img.size[1])   \n",
    "  # Grayscale, Gaussian blur, Otsu's threshold\n",
    "  image = img #.crop((left, top, right, bottom))\n",
    "  image = load_image_into_numpy_array(image)\n",
    "  cv2.imwrite(os.path.join(model_dir,'orgcrop.jpg'),image)\n",
    "  image = cv2.resize(image,(0,0),fx=7,fy=7)\n",
    "  plt.imshow(image)\n",
    "  cv2.imwrite(os.path.join(model_dir,'resized.jpg'),image)\n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  cv2.imwrite(os.path.join(model_dir,'grayscale.jpg'),gray)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(gray)\n",
    "  blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "  cv2.imwrite(os.path.join(model_dir,'GaussianBlur.jpg'),blur)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(blur)\n",
    "  #thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "  #plt.figure(figsize=IMAGE_SIZE)\n",
    "  #plt.imshow(thresh)\n",
    "\n",
    "  # Morph open to remove noise and invert image\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "  opening = cv2.morphologyEx(blur, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "  cv2.imwrite(os.path.join(model_dir,'morphologyEx.jpg'),opening)\n",
    "  invert = 255 - opening\n",
    "  cv2.imwrite(os.path.join(model_dir,'invert.jpg'),invert)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(invert)\n",
    "  print('first tiral')\n",
    "  #oem: OCR Engine modes => 1 - Neural nets LSTM engine only\n",
    "  #psm: Page Segmentation Mode ==> \n",
    "  #     3    Fully automatic page segmentation, but no OSD. (Default)\n",
    "  #     6    Assume a single uniform block of text.\n",
    "  #     7    Treat the image as a single text line.\n",
    "  print(pytesseract.image_to_string(invert, lang='eng', config='--psm 10 --oem 3 -c tessedit_char_whitelist=0123456789'))\n",
    "  print('second trial',pytesseract.image_to_string(invert, config=\"-c tessedit_char_whitelist=0123456789 -oem 0\"))\n",
    "  print('try different read config')\n",
    "\n",
    "\n",
    "\t# in order to apply Tesseract v4 to OCR text we must supply\n",
    "\t# (1) a language, \n",
    "  # (2) an OEM flag of 1, indicating that the we\n",
    "\t# wish to use the LSTM neural net model for OCR,\n",
    "\t# (3) an OEM value, in this case, 7 which implies that we are\n",
    "\t# treating the ROI as a single line of text\n",
    "  config = (\"-l eng --oem 1 --psm 7\")\n",
    "  text = pytesseract.image_to_string(image, config=config)\n",
    "\n",
    "\t# strip out non-ASCII text so we can draw the text on the image\n",
    "\t# using OpenCV, then draw the text and a bounding box surrounding\n",
    "\t# the text region of the input image\n",
    "  #text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "  \n",
    "  print('latest article::::',text)\n",
    "  text = pytesseract.image_to_string(image, config=config)\n",
    "\n",
    "  # Perform text extraction\n",
    "  #for i in [1,3,4,5,6,7,8,9,10,11,12,13]:\n",
    "  #  data = pytesseract.image_to_string(invert, lang='eng', config='--psm '+str(i))\n",
    "  #  print('--psm '+str(i), data)\n",
    "  \n",
    "  print('antoher tuning')\n",
    "  retval, image = cv2.threshold(image,200,255, cv2.THRESH_BINARY)\n",
    "  cv2.imwrite(os.path.join(model_dir,'THRESH_BINARY.jpg'),image)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image)\n",
    "  image = cv2.GaussianBlur(image,(11,11),0)\n",
    "  cv2.imwrite(os.path.join(model_dir,'GaussianBlur2.jpg'),image)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image)\n",
    "  image = cv2.medianBlur(image,9)\n",
    "  cv2.imwrite(os.path.join(model_dir,'medianBlur.jpg'),image)\n",
    "  plt.figure(figsize=IMAGE_SIZE)\n",
    "  plt.imshow(image)\n",
    "  data = pytesseract.image_to_string(image, lang='eng')\n",
    "  print('last trial',pytesseract.image_to_string(image, config=\"-c tessedit_char_whitelist=0123456789 -oem 0\"))\n",
    "  \n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def read_image_text(img, plot=False):\n",
    "  readings = []\n",
    "  config = (\"-l eng --oem 1 --psm 7\")\n",
    "  #(left, top, right, bottom) = (0,25,img.size[0],img.size[1])   \n",
    "  # Grayscale, Gaussian blur, Otsu's threshold\n",
    "  image = img #.crop((left, top, right, bottom))\n",
    "  image = load_image_into_numpy_array(image)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'orgcrop.jpg'),image)\n",
    "  #print('original image',pytesseract.image_to_string(image, config=config))\n",
    "  readings.append(pytesseract.image_to_string(image, config=config))\n",
    "  \n",
    "  image = cv2.resize(image,(0,0),fx=7,fy=7)\n",
    "  if plot:\n",
    "    plt.imshow(image)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'resized.jpg'),image)\n",
    "  #print('rescale image',pytesseract.image_to_string(image, config=config))\n",
    "  readings.append(pytesseract.image_to_string(image, config=config))\n",
    "  \n",
    "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'grayscale.jpg'),gray)\n",
    "  if plot:\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(gray)\n",
    "  #print('gray scale',pytesseract.image_to_string(gray, config=config))\n",
    "  readings.append(pytesseract.image_to_string(gray, config=config))\n",
    "\n",
    "  blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'GaussianBlur.jpg'),blur)\n",
    "  if plot:\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(blur)\n",
    "  #print('blur',pytesseract.image_to_string(blur, config=config))\n",
    "  readings.append(pytesseract.image_to_string(blur, config=config))\n",
    "\n",
    "  #thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "  #plt.figure(figsize=IMAGE_SIZE)\n",
    "  #plt.imshow(thresh)\n",
    "\n",
    "  # Morph open to remove noise and invert image\n",
    "  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "  opening = cv2.morphologyEx(blur, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'morphologyEx.jpg'),opening)\n",
    "  #print('morphologyEx',pytesseract.image_to_string(kernel, config=config))\n",
    "  readings.append(pytesseract.image_to_string(kernel, config=config))\n",
    "\n",
    "  invert = 255 - opening\n",
    "  #cv2.imwrite(os.path.join(model_dir,'invert.jpg'),invert)\n",
    "  if plot:\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(invert)\n",
    "  #print('invert',pytesseract.image_to_string(invert, config=config))\n",
    "  readings.append(pytesseract.image_to_string(invert, config=config))\n",
    "  \n",
    "  #print('another tuning')\n",
    "  retval, image = cv2.threshold(image,200,255, cv2.THRESH_BINARY)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'THRESH_BINARY.jpg'),image)\n",
    "  if plot:\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image)\n",
    "  #print('THRESH_BINARY',pytesseract.image_to_string(image, config=config))\n",
    "  readings.append(pytesseract.image_to_string(image, config=config))\n",
    "\n",
    "  image = cv2.GaussianBlur(image,(11,11),0)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'GaussianBlur2.jpg'),image)\n",
    "  if plot:\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image)\n",
    "  #print('GaussianBlur2',pytesseract.image_to_string(image, config=config))\n",
    "  readings.append(pytesseract.image_to_string(image, config=config))\n",
    "\n",
    "  image = cv2.medianBlur(image,9)\n",
    "  #cv2.imwrite(os.path.join(model_dir,'medianBlur.jpg'),image)\n",
    "  if plot:\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image)\n",
    "  #print('medianBlur',pytesseract.image_to_string(image, config=config))\n",
    "  readings.append(pytesseract.image_to_string(image, config=config))\n",
    "\n",
    "  data = pytesseract.image_to_string(image, lang='eng')\n",
    "  #print('last trial',data)\n",
    "  #print('last trial',pytesseract.image_to_string(image, config=\"-c tessedit_char_whitelist=0123456789 -oem 1\"))\n",
    "  readings.append(pytesseract.image_to_string(image, config=\"-c tessedit_char_whitelist=0123456789 -oem 1\"))\n",
    "  \n",
    "\n",
    "  return readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1588186309836,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "PKs90MNgbD9i",
    "outputId": "e2c3bfcf-6feb-45e8-dfd6-be38b63e1f01"
   },
   "outputs": [],
   "source": [
    "# running inferences.  This should show images with bounding boxes\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "\n",
    "consolidated_results = []\n",
    "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
    "\n",
    "# Set the default threshold for accepting the bounding box\n",
    "min_score_thresh=.5\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    print('processing image: ',image_path)\n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Actual detection\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    \n",
    "    # Read detection Results:\n",
    "    # Predicted boxes for crop coordinates \n",
    "    # box coordinates (ymin, xmin, ymax, xmax) are relative to the image\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    # Scores needed to filtered accepted objects based on threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks'),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=2)\n",
    "    \n",
    "    #plt.figure(figsize=IMAGE_SIZE)\n",
    "    #plt.imshow(image_np)\n",
    "\n",
    "    # Reset image array for content reading\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        # Filter based on threshold\n",
    "        if scores is None or scores[i] > min_score_thresh:\n",
    "            # boxes[i] is the box which will be drawn\n",
    "            ymin, xmin, ymax, xmax = tuple(boxes[i].tolist())\n",
    "            class_name = category_index[output_dict['detection_classes'][i]]['name']\n",
    "\n",
    "            bounding_box_coordinates = get_bounding_box_coordinates(image, ymin, xmin, ymax, xmax)\n",
    "\n",
    "            # Crop image based on bounding box\n",
    "            cropped_img = image.crop(bounding_box_coordinates)\n",
    "            #plt.figure(figsize=IMAGE_SIZE)\n",
    "            #plt.imshow(cropped_img)\n",
    "\n",
    "            #read text from cropped image\n",
    "            readings = read_image_text(cropped_img)\n",
    "            print('row',[os.path.basename(image_path),class_name,scores[i]]+readings)\n",
    "            consolidated_results.append([os.path.basename(image_path),class_name,scores[i]]+readings)\n",
    "\n",
    "            #print (\"This box is gonna get used\", ymin, xmin, ymax, xmax ,boxes[i], output_dict['detection_classes'][i])\n",
    "            #im_width, im_height = image.size\n",
    "\n",
    "df = pandas.DataFrame(consolidated_results,columns=['image','object','Score','original','resized','grayscale','GaussianBlur','morphologyEx','invert','THRESH_BINARY','GaussianBlur2','medianBlur','numOnly'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1588186309837,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "0TbJnVkm1-h_",
    "outputId": "a1793751-c673-4716-ed46-96f97f645e1a"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BWxid1NX99mi"
   },
   "source": [
    "####Use Google Vision API\n",
    "\n",
    "https://cloud.google.com/vision/docs/ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1588186309837,
     "user": {
      "displayName": "Ghawady Ehmaid",
      "photoUrl": "https://lh5.googleusercontent.com/-BPFWT2oo2Cw/AAAAAAAAAAI/AAAAAAAAAAM/fLD958Ym-60/s64/photo.jpg",
      "userId": "01223438190874405902"
     },
     "user_tz": -600
    },
    "id": "WM3QMjoJCU6D",
    "outputId": "5bde5487-a3ad-4a06-9950-2de579049ed9"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGRQXq8sAkTR"
   },
   "source": [
    "**Getting a Google API Credential**\n",
    "\n",
    "Visit API console, choose \"Credentials\" on the left-hand menu. Choose \"Create Credentials\" and generate an API key for your application. Ideally restrict it by IP/domain, but for now, just left if blank.\n",
    "Enter the key in this first executable cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8xUhqh1-uhi"
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "# API Key \n",
    "APIKEY = getpass.getpass()\n",
    "#Need to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XX1d9huyDWDu"
   },
   "outputs": [],
   "source": [
    "# Import the base64 encoding library.\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Get base64 encoding of image data\n",
    "def encode_image(image):\n",
    "  buffered = BytesIO()\n",
    "  image.save(buffered, format=\"JPEG\")\n",
    "  img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "  return img_str\n",
    "\n",
    "# Run Vision API\n",
    "def gvision_detect_text(img):\n",
    "  from googleapiclient.discovery import build\n",
    "  vservice = build('vision', 'v1', developerKey=APIKEY)\n",
    "  request = vservice.images().annotate(body={\n",
    "          'requests': [{\n",
    "                  'image': {\n",
    "                      'content': encode_image(img)\n",
    "                  },\n",
    "                  'features': [{\n",
    "                      'type': 'TEXT_DETECTION',\n",
    "                      'maxResults': 3\n",
    "                  }]\n",
    "              }],\n",
    "          })\n",
    "  responses = request.execute(num_retries=3)\n",
    "  try:\n",
    "    return responses['responses'][0]['textAnnotations'][0]['description']\n",
    "  except:\n",
    "    print('Error occured while fetching results from API')\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzp91b7WU17m"
   },
   "outputs": [],
   "source": [
    "# Process text read to fetch only the standard plate number without surrounding text\n",
    "# https://en.wikipedia.org/wiki/Vehicle_registration_plates_of_Australia\n",
    "import re\n",
    "\n",
    "def process_plateno_text(text):\n",
    "  # TODO: Need enhacement, for now only identify two sets of alphanumeric with space\n",
    "  m = re.search('( ?[a-zA-Z0-9]){1,9} ( ?[a-zA-Z0-9]){1,9}', text)\n",
    "  if m:\n",
    "      return m.group()\n",
    "  return ''\n",
    "\n",
    "def process_busrun_text(text):\n",
    "  # Filter only numbers\n",
    "  m = re.search('( ?[0-9]){1,9}', text)\n",
    "  if m:\n",
    "      return m.group()\n",
    "  return ''\n",
    "\n",
    "print('plateno',process_plateno_text('ngscars\\n10V 4VX\\nVIC\\nVICTORIA THE EDUCATION STATE\\n'))\n",
    "print('busrun',process_busrun_text('10V\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SC6B09UwXyd0"
   },
   "outputs": [],
   "source": [
    "# running inferences.  This should show images with bounding boxes\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas\n",
    "\n",
    "consolidated_results = []\n",
    "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
    "\n",
    "# Set the default threshold for accepting the bounding box\n",
    "min_score_thresh=.5\n",
    "\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    print('processing image: ',image_path)\n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    \n",
    "    # Read detection Results:\n",
    "    # Predicted boxes for crop coordinates \n",
    "    # box coordinates (ymin, xmin, ymax, xmax) are relative to the image\n",
    "    boxes = output_dict['detection_boxes']\n",
    "    # Scores needed to filtered accepted objects based on threshold\n",
    "    scores = output_dict['detection_scores']\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        # Filter based on threshold\n",
    "        if scores is None or scores[i] > min_score_thresh:\n",
    "            # boxes[i] is the box which will be drawn\n",
    "            ymin, xmin, ymax, xmax = tuple(boxes[i].tolist())\n",
    "            class_name = category_index[output_dict['detection_classes'][i]]['name']\n",
    "\n",
    "            bounding_box_coordinates = get_bounding_box_coordinates(image, ymin, xmin, ymax, xmax)\n",
    "\n",
    "            # Crop image based on bounding box\n",
    "            cropped_img = image.crop(bounding_box_coordinates)\n",
    "            #plt.figure(figsize=IMAGE_SIZE)\n",
    "            #plt.imshow(cropped_img)\n",
    "\n",
    "            #read text from cropped image\n",
    "            readings = gvision_detect_text(cropped_img)\n",
    "            #fine tune reading with appropriate regex\n",
    "            if class_name == 'PlateNo':\n",
    "              readings = process_plateno_text(readings)\n",
    "            elif class_name == 'BusRun':\n",
    "              readings = process_busrun_text(readings)\n",
    "\n",
    "            if readings:\n",
    "              #print('row',[os.path.basename(image_path),class_name,scores[i],readings])\n",
    "              consolidated_results.append([os.path.basename(image_path),class_name,scores[i],readings])\n",
    "            else:\n",
    "              print('nothing returned',readings)\n",
    "\n",
    "            #print (\"This box is gonna get used\", ymin, xmin, ymax, xmax ,boxes[i], output_dict['detection_classes'][i])\n",
    "            #im_width, im_height = image.size\n",
    "\n",
    "df2 = pandas.DataFrame(consolidated_results,columns=['image','object','Score','reading'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K2W8t7KkQ9qb"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqSFdHQvElks"
   },
   "source": [
    "# Section 7: Further Enhacements:\n",
    "\n",
    "- Try other object detection pretrained models from the available Zoo list\n",
    "- Adjust Training Hyper parameters\n",
    "- Train with more labeled images \n",
    "- Augment train images (different zooms, rotations, contrast, lighting) (Currently only default is used in the pipeline - horizontal flip & random crop is used)\n",
    "- Explore other multi digit models like SVHN https://arxiv.org/pdf/1312.6082.pdf https://github.com/penny4860/SVHN-deep-digit-detector\n",
    "- Build own multi digit model & train from public dataset (won't be as good as google vision)\n",
    "- Text post processing improvement (to ensure regex covers all standard AUS plates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eswamFiJLrnM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TF bus identification.ipynb",
   "provenance": [
    {
     "file_id": "1c8nptQvVA9v42v0gsjjc4bekd2s-fi1x",
     "timestamp": 1588133044222
    },
    {
     "file_id": "https://github.com/dctian/DeepPiCar/blob/master/models/object_detection/code/tensorflow_traffic_sign_detection.ipynb",
     "timestamp": 1587548201036
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
